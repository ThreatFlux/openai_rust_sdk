#![allow(missing_docs)]

use crate::models::functions::{Tool, ToolChoice};
use crate::models::gpt5::{ReasoningConfig, TextConfig};
use crate::models::responses::message_types::{
    ImageDetail, Message, MessageContent, MessageContentInput, MessageRole,
};
use crate::models::responses::schema_types::{JsonSchemaSpec, ResponseFormat};
use crate::models::responses::usage_types::{PromptTemplate, PromptVariable};
use crate::models::tools::{EnhancedTool, EnhancedToolChoice};
use crate::schema::SchemaBuilder;
use crate::{De, Ser};
use serde::{Deserialize, Serialize};
use serde_json::{json, Map, Value};
use std::collections::HashMap;

// -----------------------------------------------------------------------------
// Response Objects
// -----------------------------------------------------------------------------

/// Enumeration of high-level response statuses returned by the Responses API
#[derive(Debug, Clone, Copy, PartialEq, Eq, Ser, De)]
#[serde(rename_all = "snake_case")]
pub enum ResponseStatus {
    /// The response completed successfully
    Completed,
    /// The response was cancelled by the caller
    Cancelled,
    /// The response failed with an error
    Failed,
    /// The response is still being generated
    InProgress,
    /// The response is queued for processing
    Queued,
    /// The response completed but is missing data
    Incomplete,
}

impl Default for ResponseStatus {
    fn default() -> Self {
        Self::InProgress
    }
}

/// Error structure returned when a response fails
#[derive(Debug, Clone, Ser, De, Default)]
pub struct ResponseError {
    /// Error code if provided
    #[serde(skip_serializing_if = "Option::is_none")]
    pub code: Option<String>,
    /// Human readable description of the error
    #[serde(skip_serializing_if = "Option::is_none")]
    pub message: Option<String>,
    /// Parameter related to the error where applicable
    #[serde(skip_serializing_if = "Option::is_none")]
    pub param: Option<String>,
    /// Error category/type returned by the API
    #[serde(rename = "type", skip_serializing_if = "Option::is_none")]
    pub error_type: Option<String>,
    /// Any additional fields supplied by the service
    #[serde(flatten)]
    pub extra: HashMap<String, Value>,
}

/// Token usage metrics returned by the Responses API
#[derive(Debug, Clone, Ser, De, Default)]
pub struct ResponseUsage {
    /// Number of tokens consumed by the input context
    #[serde(default, rename = "input_tokens", alias = "prompt_tokens")]
    pub input_tokens: u32,
    /// Number of tokens generated in the output
    #[serde(default, rename = "output_tokens", alias = "completion_tokens")]
    pub output_tokens: u32,
    /// Total tokens consumed by the request
    #[serde(default)]
    pub total_tokens: u32,
    /// Detailed metrics for cached or audio input tokens
    #[serde(
        skip_serializing_if = "Option::is_none",
        rename = "input_tokens_details",
        alias = "prompt_tokens_details"
    )]
    pub input_tokens_details: Option<PromptTokenDetails>,
    /// Detailed metrics for reasoning or audio output tokens
    #[serde(
        skip_serializing_if = "Option::is_none",
        rename = "output_tokens_details",
        alias = "completion_tokens_details"
    )]
    pub output_tokens_details: Option<CompletionTokenDetails>,
}

/// Detailed prompt token information including caching and audio metrics
#[derive(Debug, Clone, Ser, De, Default)]
pub struct PromptTokenDetails {
    /// Number of tokens served from cache
    #[serde(default)]
    pub cached_tokens: u32,
    /// Optional audio token count if provided
    #[serde(skip_serializing_if = "Option::is_none")]
    pub audio_tokens: Option<u32>,
}

/// Detailed completion token information including reasoning metrics
#[derive(Debug, Clone, Ser, De, Default)]
pub struct CompletionTokenDetails {
    /// Number of reasoning tokens generated by the model
    #[serde(default)]
    pub reasoning_tokens: u32,
    /// Accepted prediction tokens for reasoning models
    #[serde(default)]
    pub accepted_prediction_tokens: u32,
    /// Rejected prediction tokens for reasoning models
    #[serde(default)]
    pub rejected_prediction_tokens: u32,
    /// Optional audio token count if provided
    #[serde(skip_serializing_if = "Option::is_none")]
    pub audio_tokens: Option<u32>,
}

/// Primary response object returned by the OpenAI Responses API
#[derive(Debug, Clone, Ser, De, Default)]
pub struct ResponseObject {
    /// Unique identifier for the response
    pub id: String,
    /// Object type â€“ should always be "response"
    pub object: String,
    /// Unix timestamp indicating when the response was created
    #[serde(default)]
    pub created_at: u64,
    /// Current processing status of the response
    #[serde(default)]
    pub status: ResponseStatus,
    /// Model used to generate the response
    #[serde(skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    /// Error details when the response failed
    #[serde(skip_serializing_if = "Option::is_none")]
    pub error: Option<ResponseError>,
    /// List of generated output items (messages, tool calls, reasoning traces, ...)
    #[serde(default)]
    pub output: Vec<ResponseItem>,
    /// Aggregated token usage metrics
    #[serde(skip_serializing_if = "Option::is_none")]
    pub usage: Option<ResponseUsage>,
    /// Convenience field aggregated by the platform for plain text responses
    #[serde(skip_serializing_if = "Option::is_none")]
    pub output_text: Option<String>,
    /// Optional metadata supplied when the response was created
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, Value>>,
    /// Any additional fields returned by the service that we do not explicitly model
    #[serde(flatten)]
    pub extra: HashMap<String, Value>,
}

impl ResponseObject {
    /// Collect all text fragments contained in output items into a single string
    #[must_use]
    pub fn output_text(&self) -> String {
        if let Some(text) = &self.output_text {
            return text.clone();
        }

        self.output
            .iter()
            .flat_map(ResponseItem::text_fragments)
            .collect::<String>()
    }

    /// Convenience accessor for the first assistant text output in the response
    #[must_use]
    pub fn first_text(&self) -> Option<String> {
        self.output
            .iter()
            .find(|item| item.item_type == "message")
            .and_then(|item| item.text_fragments().into_iter().next())
    }

    /// Whether the response completed successfully
    #[must_use]
    pub fn is_completed(&self) -> bool {
        matches!(self.status, ResponseStatus::Completed)
    }

    /// Whether the response failed with an error
    #[must_use]
    pub fn is_failed(&self) -> bool {
        matches!(self.status, ResponseStatus::Failed)
    }
}

/// Generic output or input item that is part of a response payload
#[derive(Debug, Clone, Ser, De, Default)]
pub struct ResponseItem {
    /// Type discriminator for the item (message, tool_call, reasoning, ...)
    #[serde(rename = "type")]
    pub item_type: String,
    /// Unique identifier for the item when available
    #[serde(skip_serializing_if = "Option::is_none")]
    pub id: Option<String>,
    /// Processing status for streaming items (completed, in_progress, ...)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub status: Option<String>,
    /// Role associated with message items
    #[serde(skip_serializing_if = "Option::is_none")]
    pub role: Option<MessageRole>,
    /// Rich content associated with the item
    #[serde(default)]
    pub content: Vec<ContentPart>,
    /// Additional fields depending on the item type
    #[serde(flatten)]
    pub extra: HashMap<String, Value>,
}

impl ResponseItem {
    /// Extract contiguous text fragments from the item
    pub fn text_fragments(&self) -> Vec<String> {
        self.content
            .iter()
            .filter_map(ContentPart::text_fragment)
            .collect()
    }
}

/// Individual content parts within a response item (text, images, audio, etc.)
#[derive(Debug, Clone, Ser, De, Default)]
pub struct ContentPart {
    /// Content kind as reported by the API (input_text, output_text, input_image, ...)
    #[serde(rename = "type")]
    pub part_type: String,
    /// Text payload for text-based parts
    #[serde(skip_serializing_if = "Option::is_none")]
    pub text: Option<String>,
    /// Optional annotations associated with text
    #[serde(skip_serializing_if = "Option::is_none")]
    pub annotations: Option<Vec<Annotation>>,
    /// Additional data specific to the content type
    #[serde(flatten)]
    pub extra: HashMap<String, Value>,
}

impl ContentPart {
    /// Get the text fragment from this content part
    fn text_fragment(&self) -> Option<String> {
        self.text.clone()
    }
}

/// Annotation metadata attached to text content
#[derive(Debug, Clone, Ser, De, Default)]
pub struct Annotation {
    /// Annotation kind emitted by the service
    #[serde(rename = "type")]
    pub annotation_type: String,
    /// Additional annotation attributes
    #[serde(flatten)]
    pub extra: HashMap<String, Value>,
}

// -----------------------------------------------------------------------------
// Request Builder
// -----------------------------------------------------------------------------

/// Supported response input shapes for the Responses API
#[derive(Debug, Clone, Ser, De)]
#[serde(untagged)]
pub enum ResponseInput {
    /// Simple text input
    Text(String),
    /// Conversation-style input messages
    Messages(Vec<Message>),
    /// Pre-built request payload
    Raw(Value),
}

impl Default for ResponseInput {
    fn default() -> Self {
        Self::Text(String::new())
    }
}

impl From<String> for ResponseInput {
    fn from(value: String) -> Self {
        Self::Text(value)
    }
}

impl From<&str> for ResponseInput {
    fn from(value: &str) -> Self {
        Self::Text(value.to_string())
    }
}

impl From<Vec<Message>> for ResponseInput {
    fn from(messages: Vec<Message>) -> Self {
        Self::Messages(messages)
    }
}

impl From<Value> for ResponseInput {
    fn from(value: Value) -> Self {
        Self::Raw(value)
    }
}

/// Instructions can be supplied either as plain text or structured messages
#[derive(Debug, Clone, Ser, De)]
#[serde(untagged)]
pub enum Instructions {
    /// Simple textual instructions
    Text(String),
    /// Structured instruction messages
    Messages(Vec<Message>),
    /// Raw custom payload
    Raw(Value),
}

impl From<String> for Instructions {
    fn from(value: String) -> Self {
        Self::Text(value)
    }
}

impl From<&str> for Instructions {
    fn from(value: &str) -> Self {
        Self::Text(value.to_string())
    }
}

impl From<Vec<Message>> for Instructions {
    fn from(messages: Vec<Message>) -> Self {
        Self::Messages(messages)
    }
}

impl From<Value> for Instructions {
    fn from(value: Value) -> Self {
        Self::Raw(value)
    }
}

/// Desired service tier for serving a response request
#[derive(Debug, Clone, Copy, Ser, De, PartialEq, Eq, Default)]
#[serde(rename_all = "snake_case")]
pub enum ServiceTier {
    /// Use the project default tier
    #[default]
    Auto,
    /// Standard pricing/performance
    Default,
    /// Flexible processing tier
    Flex,
    /// Priority tier for low latency
    Priority,
}

/// Streaming options controlling the SSE payload
#[derive(Debug, Clone, Ser, De, Default)]
pub struct StreamOptions {
    /// Whether to include usage metrics mid-stream
    #[serde(skip_serializing_if = "Option::is_none")]
    pub include_usage: Option<bool>,
    /// Filter for event types to be delivered
    #[serde(skip_serializing_if = "Option::is_none")]
    pub event_types: Option<Vec<String>>,
    /// Enable or disable stream obfuscation
    #[serde(skip_serializing_if = "Option::is_none")]
    pub include_obfuscation: Option<bool>,
    /// Continue streaming after a particular event sequence number
    #[serde(skip_serializing_if = "Option::is_none")]
    pub starting_after: Option<u64>,
    /// Catch-all for forward compatibility
    #[serde(flatten)]
    pub extra: HashMap<String, Value>,
}

/// Reference to an existing or inline conversation definition
#[derive(Debug, Clone, Ser, De)]
#[serde(untagged)]
pub enum ConversationReference {
    /// Reference a previously created conversation by ID
    Id(String),
    /// Supply conversation metadata inline
    Object(ConversationObject),
}

/// Structured conversation descriptor used inline with response requests
#[derive(Debug, Clone, Ser, De, Default)]
pub struct ConversationObject {
    /// Optional conversation identifier
    #[serde(skip_serializing_if = "Option::is_none")]
    pub id: Option<String>,
    /// Optional metadata map
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,
    /// Additional conversation fields
    #[serde(flatten)]
    pub extra: HashMap<String, Value>,
}

/// Complete request payload builder for the Responses API
#[derive(Debug, Clone, Ser, De, Default)]
pub struct CreateResponseRequest {
    /// Model to use for generating the response
    #[serde(default)]
    pub model: String,
    /// Input supplied to the model
    #[serde(default)]
    pub input: ResponseInput,
    /// Optional instructions supplied with the request
    #[serde(skip_serializing_if = "Option::is_none")]
    pub instructions: Option<Instructions>,
    /// Previous response identifier for multi-turn interactions
    #[serde(skip_serializing_if = "Option::is_none")]
    pub previous_response_id: Option<String>,
    /// Conversation reference used to maintain context server-side
    #[serde(skip_serializing_if = "Option::is_none")]
    pub conversation: Option<ConversationReference>,
    /// Additional fields to include in the response body
    #[serde(skip_serializing_if = "Option::is_none")]
    pub include: Option<Vec<String>>,
    /// Optional metadata map (string key/value pairs)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,
    /// Optional safety identifier for policy tracking
    #[serde(skip_serializing_if = "Option::is_none")]
    pub safety_identifier: Option<String>,
    /// Temperature sampling parameter
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
    /// Nucleus sampling parameter
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,
    /// Top log probabilities to return
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<u8>,
    /// Frequency penalty
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>,
    /// Presence penalty
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>,
    /// Maximum number of output tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_output_tokens: Option<u32>,
    /// Maximum number of tool calls allowed
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tool_calls: Option<u32>,
    /// Whether to enable streaming
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
    /// Additional stream configuration options
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream_options: Option<StreamOptions>,
    /// Prompt template reference
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt: Option<PromptTemplate>,
    /// Prompt cache key for cache-aware routing
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt_cache_key: Option<String>,
    /// Reasoning configuration for advanced models
    #[serde(skip_serializing_if = "Option::is_none")]
    pub reasoning: Option<ReasoningConfig>,
    /// Text configuration for structured outputs
    #[serde(skip_serializing_if = "Option::is_none")]
    pub text: Option<TextConfig>,
    /// Tool definitions available to the model
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<Tool>>,
    /// Tool choice configuration
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<ToolChoice>,
    /// Enhanced tools (web search, file search, MCP, etc.)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub enhanced_tools: Option<Vec<EnhancedTool>>,
    /// Enhanced tool choice configuration
    #[serde(skip_serializing_if = "Option::is_none")]
    pub enhanced_tool_choice: Option<EnhancedToolChoice>,
    /// Whether to allow parallel tool calls
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parallel_tool_calls: Option<bool>,
    /// Service tier selection
    #[serde(skip_serializing_if = "Option::is_none")]
    pub service_tier: Option<ServiceTier>,
    /// Whether to store the response for later retrieval
    #[serde(skip_serializing_if = "Option::is_none")]
    pub store: Option<bool>,
    /// Whether to run the response generation in the background
    #[serde(skip_serializing_if = "Option::is_none")]
    pub background: Option<bool>,
    /// Response format (text, JSON object, JSON schema)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_format: Option<ResponseFormat>,
    /// Deprecated user identifier (kept for compatibility)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

impl CreateResponseRequest {
    /// Create a new response request with simple text input
    pub fn new_text(model: impl Into<String>, input: impl Into<String>) -> Self {
        Self {
            model: model.into(),
            input: ResponseInput::Text(input.into()),
            ..Self::default()
        }
    }

    /// Create a new response request with conversation messages
    pub fn new_messages(model: impl Into<String>, messages: Vec<Message>) -> Self {
        Self {
            model: model.into(),
            input: ResponseInput::Messages(messages),
            ..Self::default()
        }
    }

    /// Attach string instructions to the request
    pub fn with_instructions(mut self, instructions: impl Into<Instructions>) -> Self {
        self.instructions = Some(instructions.into());
        self
    }

    /// Attach a conversation reference by ID
    #[must_use]
    pub fn with_conversation_id(mut self, conversation_id: impl Into<String>) -> Self {
        self.conversation = Some(ConversationReference::Id(conversation_id.into()));
        self
    }

    /// Attach a conversation object with metadata to the request
    #[must_use]
    pub fn with_conversation_object(mut self, conversation: ConversationObject) -> Self {
        self.conversation = Some(ConversationReference::Object(conversation));
        self
    }

    /// Set the previous response identifier for continuation
    #[must_use]
    pub fn with_previous_response_id(mut self, response_id: impl Into<String>) -> Self {
        self.previous_response_id = Some(response_id.into());
        self
    }

    /// Include additional response sections in the payload
    #[must_use]
    pub fn with_include(mut self, include: Vec<String>) -> Self {
        self.include = Some(include);
        self
    }

    /// Set metadata key/value pairs for the response
    #[must_use]
    pub fn with_metadata(mut self, metadata: HashMap<String, String>) -> Self {
        self.metadata = Some(metadata);
        self
    }

    /// Attach a single metadata entry
    #[must_use]
    pub fn with_metadata_entry(mut self, key: impl Into<String>, value: impl Into<String>) -> Self {
        let metadata = self.metadata.get_or_insert_with(HashMap::new);
        metadata.insert(key.into(), value.into());
        self
    }

    /// Provide a safety identifier for policy tracking
    #[must_use]
    pub fn with_safety_identifier(mut self, identifier: impl Into<String>) -> Self {
        self.safety_identifier = Some(identifier.into());
        self
    }

    /// Enable streaming for the request
    #[must_use]
    pub fn with_streaming(mut self, stream: bool) -> Self {
        self.stream = Some(stream);
        self
    }

    /// Set the sampling temperature for the response
    #[must_use]
    pub fn with_temperature(mut self, temperature: f32) -> Self {
        self.temperature = Some(temperature);
        self
    }

    /// Set the nucleus sampling parameter
    #[must_use]
    pub fn with_top_p(mut self, top_p: f32) -> Self {
        self.top_p = Some(top_p);
        self
    }

    /// Apply a frequency penalty to discourage repetition
    #[must_use]
    pub fn with_frequency_penalty(mut self, penalty: f32) -> Self {
        self.frequency_penalty = Some(penalty);
        self
    }

    /// Apply a presence penalty to encourage new topics
    #[must_use]
    pub fn with_presence_penalty(mut self, penalty: f32) -> Self {
        self.presence_penalty = Some(penalty);
        self
    }

    /// Limit the number of output tokens generated
    #[must_use]
    pub fn with_max_tokens(mut self, max_tokens: u32) -> Self {
        self.max_output_tokens = Some(max_tokens);
        self
    }

    /// Set the maximum number of tool calls permitted
    #[must_use]
    pub fn with_max_tool_calls(mut self, max_tool_calls: u32) -> Self {
        self.max_tool_calls = Some(max_tool_calls);
        self
    }

    /// Configure tool definitions
    #[must_use]
    pub fn with_tools(mut self, tools: Vec<Tool>) -> Self {
        self.tools = Some(tools);
        self
    }

    /// Configure tool choice preference
    #[must_use]
    pub fn with_tool_choice(mut self, choice: ToolChoice) -> Self {
        self.tool_choice = Some(choice);
        self
    }

    /// Configure enhanced tool array
    #[must_use]
    pub fn with_enhanced_tools(mut self, tools: Vec<EnhancedTool>) -> Self {
        self.enhanced_tools = Some(tools);
        self
    }

    /// Allow the model to make parallel tool calls
    #[must_use]
    pub fn with_parallel_tool_calls(mut self, enabled: bool) -> Self {
        self.parallel_tool_calls = Some(enabled);
        self
    }

    /// Configure enhanced tool choice behaviour
    #[must_use]
    pub fn with_enhanced_tool_choice(mut self, choice: EnhancedToolChoice) -> Self {
        self.enhanced_tool_choice = Some(choice);
        self
    }

    /// Provide a prompt cache key for cache-aware routing
    #[must_use]
    pub fn with_prompt_cache_key(mut self, key: impl Into<String>) -> Self {
        self.prompt_cache_key = Some(key.into());
        self
    }

    /// Configure the service tier used to process the request
    #[must_use]
    pub fn with_service_tier(mut self, tier: ServiceTier) -> Self {
        self.service_tier = Some(tier);
        self
    }

    /// Control whether the response is persisted by the platform
    #[must_use]
    pub fn with_store(mut self, store: bool) -> Self {
        self.store = Some(store);
        self
    }

    /// Run the request in the background
    #[must_use]
    pub fn with_background(mut self, background: bool) -> Self {
        self.background = Some(background);
        self
    }

    /// Enable JSON mode responses
    #[must_use]
    pub fn with_json_mode(mut self) -> Self {
        self.response_format = Some(ResponseFormat::JsonObject);
        self
    }

    /// Enforce a JSON schema on the response
    #[must_use]
    pub fn with_json_schema(mut self, name: impl Into<String>, schema: Value) -> Self {
        self.response_format = Some(ResponseFormat::JsonSchema {
            json_schema: JsonSchemaSpec {
                name: name.into(),
                description: None,
                schema,
                strict: false,
            },
            strict: false,
        });
        self
    }

    /// Enforce a strict JSON schema on the response
    #[must_use]
    pub fn with_strict_json_schema(mut self, name: impl Into<String>, schema: Value) -> Self {
        self.response_format = Some(ResponseFormat::JsonSchema {
            json_schema: JsonSchemaSpec {
                name: name.into(),
                description: None,
                schema,
                strict: true,
            },
            strict: true,
        });
        self
    }

    /// Configure the response schema using the schema builder helper
    #[must_use]
    pub fn with_schema_builder(mut self, name: impl Into<String>, builder: SchemaBuilder) -> Self {
        let schema = builder.build();
        self.response_format = Some(ResponseFormat::JsonSchema {
            json_schema: JsonSchemaSpec {
                name: name.into(),
                description: None,
                schema: schema.to_value(),
                strict: false,
            },
            strict: false,
        });
        self
    }

    /// Convert the request into a JSON payload for transmission
    pub fn to_payload(&self) -> serde_json::Result<Value> {
        let mut payload = Map::new();
        payload.insert("model".into(), Value::String(self.model.clone()));

        self.add_input_to_payload(&mut payload)?;
        self.add_instructions_to_payload(&mut payload)?;
        self.add_context_fields_to_payload(&mut payload)?;
        self.add_generation_params_to_payload(&mut payload);
        self.add_tools_to_payload(&mut payload)?;
        self.add_optional_fields_to_payload(&mut payload)?;

        Ok(Value::Object(payload))
    }

    /// Add the input field to the payload based on the input type
    fn add_input_to_payload(&self, payload: &mut Map<String, Value>) -> serde_json::Result<()> {
        match &self.input {
            ResponseInput::Text(text) => {
                payload.insert("input".into(), Value::String(text.clone()));
            }
            ResponseInput::Messages(messages) => {
                payload.insert(
                    "input".into(),
                    Value::Array(convert_messages_to_input(messages)?),
                );
            }
            ResponseInput::Raw(value) => {
                payload.insert("input".into(), value.clone());
            }
        }
        Ok(())
    }

    /// Add the instructions field to the payload if present
    fn add_instructions_to_payload(
        &self,
        payload: &mut Map<String, Value>,
    ) -> serde_json::Result<()> {
        if let Some(instructions) = &self.instructions {
            let value = match instructions {
                Instructions::Text(text) => Value::String(text.clone()),
                Instructions::Messages(messages) => {
                    Value::Array(convert_messages_to_input(messages)?)
                }
                Instructions::Raw(value) => value.clone(),
            };
            payload.insert("instructions".into(), value);
        }
        Ok(())
    }

    /// Add context-related fields to the payload (conversation, metadata, etc.)
    fn add_context_fields_to_payload(
        &self,
        payload: &mut Map<String, Value>,
    ) -> serde_json::Result<()> {
        if let Some(previous) = &self.previous_response_id {
            payload.insert(
                "previous_response_id".into(),
                Value::String(previous.clone()),
            );
        }

        if let Some(conversation) = &self.conversation {
            payload.insert("conversation".into(), serde_json::to_value(conversation)?);
        }

        if let Some(include) = &self.include {
            payload.insert("include".into(), serde_json::to_value(include)?);
        }

        if let Some(metadata) = &self.metadata {
            payload.insert("metadata".into(), serde_json::to_value(metadata)?);
        }

        if let Some(identifier) = &self.safety_identifier {
            payload.insert(
                "safety_identifier".into(),
                Value::String(identifier.clone()),
            );
        }
        Ok(())
    }

    /// Add generation parameters to the payload (temperature, penalties, etc.)
    fn add_generation_params_to_payload(&self, payload: &mut Map<String, Value>) {
        if let Some(temp) = self.temperature {
            payload.insert("temperature".into(), json!(temp));
        }

        if let Some(top_p) = self.top_p {
            payload.insert("top_p".into(), json!(top_p));
        }

        if let Some(top_logprobs) = self.top_logprobs {
            payload.insert("top_logprobs".into(), json!(top_logprobs));
        }

        if let Some(freq_penalty) = self.frequency_penalty {
            payload.insert("frequency_penalty".into(), json!(freq_penalty));
        }

        if let Some(pres_penalty) = self.presence_penalty {
            payload.insert("presence_penalty".into(), json!(pres_penalty));
        }

        if let Some(max_tokens) = self.max_output_tokens {
            payload.insert("max_output_tokens".into(), json!(max_tokens));
        }

        if let Some(max_tool_calls) = self.max_tool_calls {
            payload.insert("max_tool_calls".into(), json!(max_tool_calls));
        }
    }

    /// Add tools and tool choice configuration to the payload
    fn add_tools_to_payload(&self, payload: &mut Map<String, Value>) -> serde_json::Result<()> {
        let mut tools: Vec<Value> = Vec::new();
        if let Some(tool_list) = &self.tools {
            for tool in tool_list {
                tools.push(serde_json::to_value(tool)?);
            }
        }
        if let Some(enhanced_list) = &self.enhanced_tools {
            for tool in enhanced_list {
                tools.push(serde_json::to_value(tool)?);
            }
        }
        if !tools.is_empty() {
            payload.insert("tools".into(), Value::Array(tools));
        }

        if let Some(choice) = &self.tool_choice {
            payload.insert("tool_choice".into(), serde_json::to_value(choice)?);
        } else if let Some(choice) = &self.enhanced_tool_choice {
            payload.insert("tool_choice".into(), serde_json::to_value(choice)?);
        }

        if let Some(parallel) = self.parallel_tool_calls {
            payload.insert("parallel_tool_calls".into(), json!(parallel));
        }
        Ok(())
    }

    /// Add remaining optional fields to the payload
    fn add_optional_fields_to_payload(
        &self,
        payload: &mut Map<String, Value>,
    ) -> serde_json::Result<()> {
        if let Some(stream) = self.stream {
            payload.insert("stream".into(), json!(stream));
        }

        if let Some(options) = &self.stream_options {
            payload.insert("stream_options".into(), serde_json::to_value(options)?);
        }

        if let Some(prompt) = &self.prompt {
            payload.insert("prompt".into(), serde_json::to_value(prompt)?);
        }

        if let Some(cache_key) = &self.prompt_cache_key {
            payload.insert("prompt_cache_key".into(), Value::String(cache_key.clone()));
        }

        if let Some(reasoning) = &self.reasoning {
            payload.insert("reasoning".into(), serde_json::to_value(reasoning)?);
        }

        if let Some(text) = &self.text {
            payload.insert("text".into(), serde_json::to_value(text)?);
        }

        if let Some(tier) = &self.service_tier {
            payload.insert("service_tier".into(), serde_json::to_value(tier)?);
        }

        if let Some(store) = self.store {
            payload.insert("store".into(), json!(store));
        }

        if let Some(background) = self.background {
            payload.insert("background".into(), json!(background));
        }

        if let Some(format) = &self.response_format {
            let value = serialize_response_format(format);
            if !value.is_null() {
                payload.insert("response_format".into(), value);
            }
        }

        if let Some(user) = &self.user {
            payload.insert("user".into(), Value::String(user.clone()));
        }
        Ok(())
    }
}

/// Convert an array of messages into the v1/responses input format
fn convert_messages_to_input(messages: &[Message]) -> serde_json::Result<Vec<Value>> {
    messages.iter().map(message_to_input_item).collect()
}

/// Convert a single message to a JSON value for the v1/responses API
fn message_to_input_item(message: &Message) -> serde_json::Result<Value> {
    let mut item = Map::new();
    item.insert("type".into(), Value::String("message".to_string()));
    item.insert(
        "role".into(),
        Value::String(message_role_as_str(&message.role).to_string()),
    );
    item.insert(
        "content".into(),
        Value::Array(convert_message_content(&message.content)?),
    );
    Ok(Value::Object(item))
}

/// Convert message content into the input format for the v1/responses API
fn convert_message_content(content: &MessageContentInput) -> serde_json::Result<Vec<Value>> {
    match content {
        MessageContentInput::Text(text) => Ok(vec![json!({
            "type": "input_text",
            "text": text
        })]),
        MessageContentInput::Array(contents) => contents
            .iter()
            .map(|part| match part {
                MessageContent::Text { text } => Ok(json!({
                    "type": "input_text",
                    "text": text
                })),
                MessageContent::Image { image_url } => {
                    let mut image = Map::new();
                    image.insert("type".into(), Value::String("input_image".into()));
                    let mut image_url_value = Map::new();
                    image_url_value.insert("url".into(), Value::String(image_url.url.clone()));
                    if let Some(detail) = &image_url.detail {
                        image_url_value.insert(
                            "detail".into(),
                            Value::String(image_detail_to_str(detail).to_string()),
                        );
                    }
                    image.insert("image_url".into(), Value::Object(image_url_value));
                    Ok(Value::Object(image))
                }
            })
            .collect(),
    }
}

/// Convert a MessageRole enum to its string representation
fn message_role_as_str(role: &MessageRole) -> &'static str {
    match role {
        MessageRole::Developer => "developer",
        MessageRole::System => "system",
        MessageRole::User => "user",
        MessageRole::Assistant => "assistant",
    }
}

/// Convert an ImageDetail enum to its string representation
fn image_detail_to_str(detail: &ImageDetail) -> &'static str {
    match detail {
        ImageDetail::Auto => "auto",
        ImageDetail::Low => "low",
        ImageDetail::High => "high",
    }
}

/// Serialize a ResponseFormat enum into its JSON representation
fn serialize_response_format(format: &ResponseFormat) -> Value {
    match format {
        ResponseFormat::Text => Value::Null,
        ResponseFormat::JsonObject => json!({ "type": "json_object" }),
        ResponseFormat::JsonSchema {
            json_schema,
            strict,
        } => json!({
            "type": "json_schema",
            "json_schema": {
                "name": json_schema.name,
                "description": json_schema.description,
                "schema": json_schema.schema,
                "strict": json_schema.strict || *strict
            }
        }),
    }
}

/// Convert a legacy `ResponseRequest` (chat completions style) into a modern request
pub fn from_legacy_request(
    request: &crate::models::responses::ResponseRequest,
) -> CreateResponseRequest {
    let instructions = request
        .instructions
        .as_ref()
        .map(|text| Instructions::Text(text.clone()));

    let input = match &request.input {
        crate::models::responses::ResponseInput::Text(text) => ResponseInput::Text(text.clone()),
        crate::models::responses::ResponseInput::Messages(messages) => {
            ResponseInput::Messages(messages.clone())
        }
    };

    CreateResponseRequest {
        model: request.model.clone(),
        input,
        instructions,
        previous_response_id: request.previous_response_id.clone(),
        reasoning: request.reasoning.clone(),
        text: request.text.clone(),
        temperature: request.temperature,
        top_p: request.top_p,
        frequency_penalty: request.frequency_penalty,
        presence_penalty: request.presence_penalty,
        max_output_tokens: request.max_tokens,
        stream: request.stream,
        prompt: request.prompt.clone(),
        prompt_cache_key: request.prompt_cache_key.clone(),
        tools: request.tools.clone(),
        tool_choice: request.tool_choice.clone(),
        enhanced_tools: request.enhanced_tools.clone(),
        enhanced_tool_choice: request.enhanced_tool_choice.clone(),
        parallel_tool_calls: request.parallel_tool_calls,
        response_format: request.response_format.clone(),
        ..CreateResponseRequest::default()
    }
}

/// Convert a modern `ResponseObject` into the legacy `ResponseResult` structure
pub fn to_legacy_response(response: &ResponseObject) -> crate::models::responses::ResponseResult {
    use crate::models::responses::{
        ResponseChoice as LegacyResponseChoice, ResponseOutput as LegacyResponseOutput,
        ResponseResult as LegacyResponseResult, Usage as LegacyUsage,
    };

    let text = response.output_text();
    let legacy_output = LegacyResponseOutput {
        content: Some(text),
        tool_calls: None,
        function_calls: None,
        structured_data: None,
        schema_validation: None,
    };

    let choice = LegacyResponseChoice {
        index: 0,
        message: legacy_output,
        finish_reason: None,
    };

    let usage = response.usage.as_ref().map(|usage| LegacyUsage {
        prompt_tokens: usage.input_tokens,
        completion_tokens: usage.output_tokens,
        total_tokens: usage.total_tokens,
        prompt_tokens_details: usage.input_tokens_details.as_ref().map(|details| {
            crate::models::responses::usage_types::PromptTokenDetails {
                cached_tokens: details.cached_tokens,
                audio_tokens: details.audio_tokens,
            }
        }),
        completion_tokens_details: usage.output_tokens_details.as_ref().map(|details| {
            crate::models::responses::usage_types::CompletionTokenDetails {
                reasoning_tokens: details.reasoning_tokens,
                accepted_prediction_tokens: details.accepted_prediction_tokens,
                rejected_prediction_tokens: details.rejected_prediction_tokens,
                audio_tokens: details.audio_tokens,
            }
        }),
    });

    LegacyResponseResult {
        id: Some(response.id.clone()),
        object: response.object.clone(),
        created: response.created_at,
        model: response.model.clone().unwrap_or_default(),
        choices: vec![choice],
        usage,
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::functions::{FunctionTool, Tool, ToolChoice};
    use crate::models::responses::ResponseRequest as LegacyResponseRequest;
    use crate::models::tools::{EnhancedTool, EnhancedToolChoice};
    use crate::schema::SchemaBuilder;
    use serde_json::json;

    #[test]
    fn converts_legacy_text_request() {
        let legacy = LegacyResponseRequest::new_text("gpt-test", "Hello world")
            .with_temperature(0.7)
            .with_max_tokens(256)
            .with_instructions("Be helpful");

        let modern = from_legacy_request(&legacy);
        assert_eq!(modern.model, "gpt-test");
        match modern.input {
            ResponseInput::Text(ref text) => assert_eq!(text, "Hello world"),
            _ => panic!("expected text input"),
        }
        assert_eq!(modern.temperature, Some(0.7));
        assert_eq!(modern.max_output_tokens, Some(256));
        assert!(
            matches!(modern.instructions, Some(Instructions::Text(ref s)) if s == "Be helpful")
        );
    }

    #[test]
    fn converts_response_object_to_legacy() {
        let response = ResponseObject {
            id: "resp_123".to_string(),
            object: "response".to_string(),
            created_at: 42,
            status: ResponseStatus::Completed,
            model: Some("gpt-test".to_string()),
            output: vec![ResponseItem {
                item_type: "message".to_string(),
                id: Some("msg_1".to_string()),
                status: Some("completed".to_string()),
                role: Some(MessageRole::Assistant),
                content: vec![ContentPart {
                    part_type: "output_text".to_string(),
                    text: Some("Hi there".to_string()),
                    annotations: None,
                    extra: HashMap::new(),
                }],
                extra: HashMap::new(),
            }],
            usage: Some(ResponseUsage {
                input_tokens: 12,
                output_tokens: 4,
                total_tokens: 16,
                input_tokens_details: Some(PromptTokenDetails {
                    cached_tokens: 2,
                    audio_tokens: None,
                }),
                output_tokens_details: Some(CompletionTokenDetails {
                    reasoning_tokens: 1,
                    accepted_prediction_tokens: 0,
                    rejected_prediction_tokens: 0,
                    audio_tokens: None,
                }),
            }),
            ..ResponseObject::default()
        };

        let legacy = to_legacy_response(&response);
        assert_eq!(legacy.id.as_deref(), Some("resp_123"));
        assert_eq!(legacy.model, "gpt-test");
        assert_eq!(legacy.choices.len(), 1);
        assert_eq!(legacy.output_text(), "Hi there");
        let usage = legacy.usage.expect("usage");
        assert_eq!(usage.prompt_tokens, 12);
        assert_eq!(usage.completion_tokens, 4);
        assert_eq!(usage.total_tokens, 16);
        assert_eq!(
            usage
                .prompt_tokens_details
                .as_ref()
                .map(|d| d.cached_tokens)
                .unwrap_or_default(),
            2
        );
    }

    #[test]
    fn request_to_payload_includes_optional_fields() {
        let mut metadata = HashMap::new();
        metadata.insert("category".into(), "demo".into());

        let mut request = CreateResponseRequest::new_text("gpt-test", "payload check")
            .with_instructions(Instructions::Text("Follow guidelines".into()))
            .with_conversation_id("conv_123")
            .with_include(vec!["output".into()])
            .with_metadata(metadata)
            .with_safety_identifier("user-42")
            .with_temperature(0.5)
            .with_top_p(0.9)
            .with_frequency_penalty(0.1)
            .with_presence_penalty(0.2)
            .with_max_tokens(512)
            .with_parallel_tool_calls(true)
            .with_prompt_cache_key("cache-key")
            .with_service_tier(ServiceTier::Flex)
            .with_store(false)
            .with_background(true)
            .with_json_mode();

        let schema_value = SchemaBuilder::object()
            .property("name", SchemaBuilder::string())
            .build()
            .to_value();
        request = request.with_json_schema("UserObject", schema_value);

        let payload = request.to_payload().expect("payload");
        let map = payload.as_object().expect("map");
        assert_eq!(map["model"], Value::String("gpt-test".into()));
        let temperature = map["temperature"].as_f64().unwrap();
        assert!((temperature - 0.5).abs() < 1e-6);
        let top_p = map["top_p"].as_f64().unwrap();
        assert!((top_p - 0.9).abs() < 1e-6);
        assert_eq!(map["max_output_tokens"], Value::from(512));
        assert_eq!(map["service_tier"], Value::String("flex".into()));
        assert_eq!(map["store"], Value::Bool(false));
        assert_eq!(map["background"], Value::Bool(true));
        assert_eq!(map["prompt_cache_key"], Value::String("cache-key".into()));
        assert!(map.contains_key("response_format"));
        assert!(map.contains_key("include"));
        assert!(map.contains_key("metadata"));
    }

    #[test]
    fn message_input_conversion_supports_multimodal() {
        let messages = vec![Message {
            role: MessageRole::User,
            content: MessageContentInput::Array(vec![
                MessageContent::text("Hello"),
                MessageContent::image_url_with_detail(
                    "https://example.com/cat.png",
                    ImageDetail::High,
                ),
            ]),
        }];
        let request = CreateResponseRequest::new_messages("gpt", messages);
        let payload = request.to_payload().unwrap();
        dbg!(&payload);
        let input_array = payload
            .get("input")
            .and_then(|v| v.as_array())
            .expect("array");
        assert_eq!(input_array.len(), 1);
        let content = input_array[0]
            .get("content")
            .and_then(|v| v.as_array())
            .expect("content array");
        assert_eq!(content.len(), 2);
        assert_eq!(content[0]["type"], Value::String("input_text".into()));
        assert_eq!(
            content[1]["image_url"]["detail"],
            Value::String("high".into())
        );
    }

    #[test]
    fn builder_serializes_tools_and_stream_options() {
        let function_tool = Tool::Function {
            function: FunctionTool::new("test_fn", "does testing", json!({})),
        };

        let stream_options = StreamOptions {
            include_usage: Some(true),
            event_types: Some(vec!["response.output_text.delta".into()]),
            include_obfuscation: Some(false),
            starting_after: Some(42),
            ..Default::default()
        };

        let mut conversation_meta = HashMap::new();
        conversation_meta.insert("topic".into(), "demo".into());
        let conversation = ConversationObject {
            id: Some("conv_42".into()),
            metadata: Some(conversation_meta),
            extra: HashMap::new(),
        };

        let messages = vec![Message {
            role: MessageRole::Developer,
            content: MessageContentInput::Text("Follow spec".into()),
        }];

        let builder = SchemaBuilder::object();

        let mut request = CreateResponseRequest::new_messages("gpt-tool", messages)
            .with_conversation_object(conversation)
            .with_previous_response_id("resp_prev")
            .with_tools(vec![function_tool])
            .with_enhanced_tools(vec![EnhancedTool::WebSearchPreview])
            .with_tool_choice(ToolChoice::Required)
            .with_enhanced_tool_choice(EnhancedToolChoice::Required)
            .with_metadata_entry("env", "test")
            .with_max_tool_calls(2)
            .with_streaming(true)
            .with_temperature(0.2)
            .with_top_p(0.8)
            .with_schema_builder("Demo", builder)
            .with_prompt_cache_key("builder-cache");

        request.stream_options = Some(stream_options.clone());

        let payload = request.to_payload().unwrap();
        let map = payload.as_object().expect("payload map");
        assert_eq!(
            map["previous_response_id"],
            Value::String("resp_prev".into())
        );
        assert_eq!(map["tools"].as_array().unwrap().len(), 2);
        assert!(map.contains_key("tool_choice"));
        let stream_map = map["stream_options"].as_object().unwrap();
        assert!(stream_map["include_usage"].as_bool().unwrap());
        assert_eq!(stream_map["starting_after"], Value::Number(42.into()));
        let conversation_map = map["conversation"].as_object().unwrap();
        assert_eq!(conversation_map["id"], Value::String("conv_42".into()));
        assert!(map["metadata"].as_object().unwrap().get("env").is_some());
    }

    #[test]
    fn from_legacy_request_converts_messages_and_tools() {
        let mut legacy = LegacyResponseRequest::new_messages(
            "gpt-legacy",
            vec![Message::user("Hello"), Message::assistant("World")],
        );
        legacy.previous_response_id = Some("resp_1".into());
        legacy.prompt_cache_key = Some("cache".into());
        legacy.tools = Some(vec![Tool::Function {
            function: FunctionTool::new("fn", "desc", json!({"type":"object"})),
        }]);
        legacy.parallel_tool_calls = Some(false);
        legacy.enhanced_tools = Some(vec![EnhancedTool::WebSearchPreview]);
        legacy.enhanced_tool_choice = Some(EnhancedToolChoice::Required);
        legacy.response_format = Some(ResponseFormat::JsonObject);
        legacy.instructions = Some("Prompt instruction".into());

        let modern = from_legacy_request(&legacy);
        assert_eq!(modern.previous_response_id.as_deref(), Some("resp_1"));
        assert_eq!(modern.prompt_cache_key.as_deref(), Some("cache"));
        assert_eq!(modern.tools.as_ref().unwrap().len(), 1);
        assert_eq!(modern.enhanced_tools.as_ref().unwrap().len(), 1);
        assert!(matches!(
            modern.enhanced_tool_choice,
            Some(EnhancedToolChoice::Required)
        ));
        assert_eq!(modern.stream, None);
        assert!(matches!(modern.instructions, Some(Instructions::Text(_))));
    }

    #[test]
    fn response_object_helpers_cover_branches() {
        let item = ResponseItem {
            item_type: "message".into(),
            id: Some("msg".into()),
            status: Some("completed".into()),
            role: Some(MessageRole::Assistant),
            content: vec![ContentPart {
                part_type: "output_text".into(),
                text: Some("Hello".into()),
                annotations: None,
                extra: HashMap::new(),
            }],
            extra: HashMap::new(),
        };

        let response = ResponseObject {
            id: "resp".into(),
            object: "response".into(),
            created_at: 0,
            status: ResponseStatus::Completed,
            output: vec![item.clone()],
            usage: None,
            metadata: None,
            output_text: None,
            model: Some("gpt".into()),
            error: None,
            extra: HashMap::new(),
        };

        assert_eq!(response.first_text(), Some("Hello".into()));
        assert!(response.is_completed());
        assert!(!response.is_failed());
        assert_eq!(response.output_text(), "Hello");

        let fragments = item.text_fragments();
        assert_eq!(fragments, vec!["Hello".to_string()]);
    }
}

// -----------------------------------------------------------------------------
// Streaming Events
// -----------------------------------------------------------------------------

/// Wrapper for SSE events returned by the Responses API
#[derive(Debug, Clone, Ser, De)]
#[serde(tag = "type")]
pub enum ResponseStreamEvent {
    /// Response creation event
    #[serde(rename = "response.created")]
    ResponseCreated {
        event_id: Option<String>,
        response: ResponseObject,
    },
    /// Response completed event (final payload)
    #[serde(rename = "response.completed")]
    ResponseCompleted {
        event_id: Option<String>,
        response: ResponseObject,
    },
    /// Response failed event with error information
    #[serde(rename = "response.failed")]
    ResponseFailed {
        event_id: Option<String>,
        response: ResponseObject,
    },
    /// Output item added to the response
    #[serde(rename = "response.output_item.added")]
    OutputItemAdded {
        event_id: Option<String>,
        response_id: String,
        output_index: u32,
        item: ResponseItem,
    },
    /// Output text delta received
    #[serde(rename = "response.output_text.delta")]
    OutputTextDelta {
        event_id: Option<String>,
        response_id: String,
        output_index: u32,
        delta: String,
    },
    /// Output text completion event
    #[serde(rename = "response.output_text.done")]
    OutputTextDone {
        event_id: Option<String>,
        response_id: String,
        output_index: u32,
        text: String,
    },
    /// Conversation item created/added event
    #[serde(rename = "conversation.item.created")]
    ConversationItemCreated {
        event_id: Option<String>,
        conversation_id: String,
        item: ResponseItem,
    },
    /// Conversation item completed event
    #[serde(rename = "conversation.item.completed")]
    ConversationItemCompleted {
        event_id: Option<String>,
        conversation_id: String,
        item: ResponseItem,
    },
    /// Rate limit update event
    #[serde(rename = "rate_limits.updated")]
    RateLimitsUpdated {
        event_id: Option<String>,
        payload: Value,
    },
    /// Error event propagated in the stream
    #[serde(rename = "error")]
    StreamError {
        event_id: Option<String>,
        error: ResponseError,
    },
    /// Fallback for forward compatibility
    #[serde(other)]
    Unknown,
}
